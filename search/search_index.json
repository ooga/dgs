{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"data-loaders/","text":"Data loaders solve the N+1 problem while loading data. The N+1 Problem Explained Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in. What If My Service Doesn\u2019t Support Loading in Batches? What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this! Implementing a Data Loader The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it. Provide as Lambda Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } MappedBatchLoader The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } Using a Data Loader The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching . Using the DgsDataFetchingEnvironment You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem. Using Spring Features such as SecurityContextHolder inside a CompletableFuture When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SecurityContextHolder work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes fetching the Principal associated with the request not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor ); Caching Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default. Batch Size Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size. Data Loader Scope Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Async Data Fetching"},{"location":"data-loaders/#the-n1-problem-explained","text":"Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in.","title":"The N+1 Problem Explained"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","text":"What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!","title":"What If My Service Doesn\u2019t Support Loading in Batches?"},{"location":"data-loaders/#implementing-a-data-loader","text":"The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.","title":"Implementing a Data Loader"},{"location":"data-loaders/#provide-as-lambda","text":"Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); }","title":"Provide as Lambda"},{"location":"data-loaders/#mappedbatchloader","text":"The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } }","title":"MappedBatchLoader"},{"location":"data-loaders/#using-a-data-loader","text":"The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching .","title":"Using a Data Loader"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","text":"You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.","title":"Using the DgsDataFetchingEnvironment"},{"location":"data-loaders/#using-spring-features-such-as-securitycontextholder-inside-a-completablefuture","text":"When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SecurityContextHolder work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes fetching the Principal associated with the request not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor );","title":"Using Spring Features such as SecurityContextHolder inside a CompletableFuture"},{"location":"data-loaders/#caching","text":"Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.","title":"Caching"},{"location":"data-loaders/#batch-size","text":"Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.","title":"Batch Size"},{"location":"data-loaders/#data-loader-scope","text":"Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Data Loader Scope"},{"location":"datafetching/","text":"In the getting started guide we introduced the @DgsData annotation, which you use to create a data fetcher. In this section, we look at some of the finer details of datafetchers. The @DgsData annotation You use the @DgsData annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a @DgsComponent class. The @DgsData annotation has two parameters: Parameter Description parentType This is the type that contains the field. field The field that the datafetcher is responsible for For example, we have the following schema. type Query { shows: [Show] } type Show { title: String actors: [Actor] } We can implement this schema with a single datafetcher. @DgsComponent public class ShowDataFetcher { @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { //Load shows from a database and return the list of Show objects return shows ; } } Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries: { shows { title } } Although we're returning Show objects, which in the example contains both a title and an actors field, the actors field gets stripped off before the response gets sent back. Child datafetchers The previous example assumed that you could load a list of Show objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the actors field doesn't get returned to the user. In such scenarios, it's better to create a separate datafetcher for the expensive field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { //Load shows, which doesn't include \"actors\" return shows ; } @DgsData ( parentType = \"Show\" , field = \"actors\" ) List < Actor > actors ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); actorsService . forShow ( show . getId ()); return actors ; } The actors datafetcher only gets executed when the actors field is included in the query. The actors datafetcher also introduces a new concept; the DgsDataFetchingEnvironment . The DgsDataFetchingEnvironment gives access to the context , the query itself, data loaders, and the source object. The source object is the object that contains the field. For this example, the source is the Show object, which you can use to get the show's identifier to use in the query for actors. Do note that the shows datafetcher is returning a list of Show , while the actors datafetcher fetches the actors for a single show. The framework executes the actors datafetcher for each Show returned by the shows datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders . Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases. Using @InputArgument It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be: An input type A scalar An enum Other types, such as output types, unions and interfaces, are not allowed as input arguments. You can get input arguments as method arguments in a datafetcher method using the @InputArgument annotation. The framework internally uses Jackson to convert the argument to the correct type. type Query { shows(title: String, filter: ShowFilter): [Show] } input ShowFilter { director: String genre: ShowGenre } enum ShowGenre { commedy, action, horror } We can write a datafetcher with the following signature: @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument String title , @InputArgument ShowFilter filter ) Optionally we can specify the name argument in the @InputArgument annotation, if the argument name doesn't match the method argument name. Nullability in Kotlin for input arguments If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception. For example: # name is a nullable input argument hello(name: String): String You must write the datafetcher function as: fun hello ( @InputArgument hello : String? ) In Java you don't have to worry about this, types can always be null. You do need to null check in your datafetching code! Using @InputArgument with lists An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the @InputArgument annotation. type Query { hello(people:[Person]): String } public String hello ( @InputArgument ( collectionType = Person . class ) List < Person > people ) Codegen constants In the examples of @DgsData so far, we used string values for the parentType and field arguments. If you are using code generation you can instead use generated constants. Codegen creates a DgsConstants class with constants for each type and field in your schema. Using this we can write a datafetcher as follows: type Query { shows: [Show] } @DgsData ( parentType = DgsConstants . QUERY_TYPE , field = DgsConstants . QUERY . Shows ) public List < Show > shows () {} The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time. @RequestHeader and DgsRequestData Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get a HTTP header value by using the @RequestHeader annotation. The @RequestHeader annotation is the same annotation as used in Spring WebMVC. public String hello ( @RequestHeader String host ) Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Alternatively, you can get the DgsRequestData object from the datafetching context. The DgsRequestData has the HTTP headers as HttpHeaders and the request itself is represented as a WebRequest . Both are types from Spring Web. Depending on your runtime environment, you can further cast the WebRequest to, for example, a ServletWebRequest . @DgsData ( parentType = \"Query\" , field = \"serverName\" ) public String serverName ( DgsDataFetchingEnvironment dfe ) { DgsRequestData requestData = DgsContext . getRequestData ( dfe ); return (( ServletWebRequest ) requestData . getWebRequest ()). getRequest (). getServerName (); } Using context The DgsRequestData object described in the previous section is part of the datafetching context . You can further customize the context for datafetchers by creating a DgsCustomContextBuilder . @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Data fetching"},{"location":"datafetching/#the-dgsdata-annotation","text":"You use the @DgsData annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a @DgsComponent class. The @DgsData annotation has two parameters: Parameter Description parentType This is the type that contains the field. field The field that the datafetcher is responsible for For example, we have the following schema. type Query { shows: [Show] } type Show { title: String actors: [Actor] } We can implement this schema with a single datafetcher. @DgsComponent public class ShowDataFetcher { @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { //Load shows from a database and return the list of Show objects return shows ; } } Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries: { shows { title } } Although we're returning Show objects, which in the example contains both a title and an actors field, the actors field gets stripped off before the response gets sent back.","title":"The @DgsData annotation"},{"location":"datafetching/#child-datafetchers","text":"The previous example assumed that you could load a list of Show objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the actors field doesn't get returned to the user. In such scenarios, it's better to create a separate datafetcher for the expensive field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { //Load shows, which doesn't include \"actors\" return shows ; } @DgsData ( parentType = \"Show\" , field = \"actors\" ) List < Actor > actors ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); actorsService . forShow ( show . getId ()); return actors ; } The actors datafetcher only gets executed when the actors field is included in the query. The actors datafetcher also introduces a new concept; the DgsDataFetchingEnvironment . The DgsDataFetchingEnvironment gives access to the context , the query itself, data loaders, and the source object. The source object is the object that contains the field. For this example, the source is the Show object, which you can use to get the show's identifier to use in the query for actors. Do note that the shows datafetcher is returning a list of Show , while the actors datafetcher fetches the actors for a single show. The framework executes the actors datafetcher for each Show returned by the shows datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders . Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases.","title":"Child datafetchers"},{"location":"datafetching/#using-inputargument","text":"It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be: An input type A scalar An enum Other types, such as output types, unions and interfaces, are not allowed as input arguments. You can get input arguments as method arguments in a datafetcher method using the @InputArgument annotation. The framework internally uses Jackson to convert the argument to the correct type. type Query { shows(title: String, filter: ShowFilter): [Show] } input ShowFilter { director: String genre: ShowGenre } enum ShowGenre { commedy, action, horror } We can write a datafetcher with the following signature: @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument String title , @InputArgument ShowFilter filter ) Optionally we can specify the name argument in the @InputArgument annotation, if the argument name doesn't match the method argument name.","title":"Using @InputArgument"},{"location":"datafetching/#nullability-in-kotlin-for-input-arguments","text":"If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception. For example: # name is a nullable input argument hello(name: String): String You must write the datafetcher function as: fun hello ( @InputArgument hello : String? ) In Java you don't have to worry about this, types can always be null. You do need to null check in your datafetching code!","title":"Nullability in Kotlin for input arguments"},{"location":"datafetching/#using-inputargument-with-lists","text":"An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the @InputArgument annotation. type Query { hello(people:[Person]): String } public String hello ( @InputArgument ( collectionType = Person . class ) List < Person > people )","title":"Using @InputArgument with lists"},{"location":"datafetching/#codegen-constants","text":"In the examples of @DgsData so far, we used string values for the parentType and field arguments. If you are using code generation you can instead use generated constants. Codegen creates a DgsConstants class with constants for each type and field in your schema. Using this we can write a datafetcher as follows: type Query { shows: [Show] } @DgsData ( parentType = DgsConstants . QUERY_TYPE , field = DgsConstants . QUERY . Shows ) public List < Show > shows () {} The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time.","title":"Codegen constants"},{"location":"datafetching/#requestheader-and-dgsrequestdata","text":"Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get a HTTP header value by using the @RequestHeader annotation. The @RequestHeader annotation is the same annotation as used in Spring WebMVC. public String hello ( @RequestHeader String host ) Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Alternatively, you can get the DgsRequestData object from the datafetching context. The DgsRequestData has the HTTP headers as HttpHeaders and the request itself is represented as a WebRequest . Both are types from Spring Web. Depending on your runtime environment, you can further cast the WebRequest to, for example, a ServletWebRequest . @DgsData ( parentType = \"Query\" , field = \"serverName\" ) public String serverName ( DgsDataFetchingEnvironment dfe ) { DgsRequestData requestData = DgsContext . getRequestData ( dfe ); return (( ServletWebRequest ) requestData . getWebRequest ()). getRequest (). getServerName (); }","title":"@RequestHeader and DgsRequestData"},{"location":"datafetching/#using-context","text":"The DgsRequestData object described in the previous section is part of the datafetching context . You can further customize the context for datafetchers by creating a DgsCustomContextBuilder . @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Using context"},{"location":"error-handling/","text":"It is common in GraphQL to support error reporting by adding an errors block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the errors block. The DGS framework has an exception handler out-of-the-box that works according to the specification described in the Error Specification section on this page. This exception handler handles exceptions from data fetchers. Any RuntimeException is translated to a GraphQLError of type INTERNAL . For some specific exception types, a more specific GraphQL error type is used. Exception type GraphQL error type description AccessDeniedException PERMISSION_DENIED When a @Secured check fails DgsEntityNotFoundException NOT_FOUND Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found Mapping custom exceptions It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } } Error specification There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema. The GraphQLError Interface The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information. The TypedError Interface Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String } The ErrorType Enumeration The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"Error Handling"},{"location":"error-handling/#mapping-custom-exceptions","text":"It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } }","title":"Mapping custom exceptions"},{"location":"error-handling/#error-specification","text":"There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema.","title":"Error specification"},{"location":"error-handling/#the-graphqlerror-interface","text":"The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information.","title":"The GraphQLError Interface"},{"location":"error-handling/#the-typederror-interface","text":"Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String }","title":"The TypedError Interface"},{"location":"error-handling/#the-errortype-enumeration","text":"The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"The ErrorType Enumeration"},{"location":"examples/","text":"We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding Github repository, and is further explained in code comments. Java DGS example : An implementation of a typical GraphQL service Kotlin DGS example : The same example as above, but implemented in Kotlin Federation examples : A Federated GraphQL example, using Apollo Gateway","title":"Examples"},{"location":"generating-code-from-schema/","text":"The DGS Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s GraphQL schema file. The plugin generates the following: Data types for types, input types, enums and interfaces. A DgsConstants class containing the names of types and fields Example data fetchers A type safe query API that represents your queries Quick Start Code generation is typically integrated in the build. A Gradle plugin has always been available, and recently a Maven plugin was made available by the community. To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]' } } apply plugin: 'com.netflix.dgs.codegen' Next, you need to add the task configuration as shown here: generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } NOTE: Please use the latest version of the plugin, available here The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. Note that on a Kotlin project, the generateJava task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin. Mapping existing types Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Date and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] } Generating Client APIs The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the QueryExecutor . The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *ProjectionRoot . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick } Generating Query APIs for external services Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client . When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API. generateJava { schemaPaths = [ \"${projectDir}/composed-schema.graphqls\" ] packageName = \"some.other.service\" generateClient = true generateDataTypes = false skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] shortProjectionNames = true maxProjectionDepth = 2 } Configuring code generation Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well. Configuration property Description Default Value schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types language Either java or kotlin Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types for primitives false (boxed types are used only for nullable fields) generateClient Generate a Query API false generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generatedSourcesDir Build directory for Gradle build outputDir Sub directory of the generatedSourcesDir to generate into generated exampleOutputDir Directory to generate datafetcher example code to generated-examples includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema skipEntityQueries Disable generating Entity queries for federated types false shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false maxProjectionDepth Maximum projection depth to generate. Useful for (federated) schemas with very deep nesting 10","title":"Code Generation"},{"location":"generating-code-from-schema/#quick-start","text":"Code generation is typically integrated in the build. A Gradle plugin has always been available, and recently a Maven plugin was made available by the community. To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]' } } apply plugin: 'com.netflix.dgs.codegen' Next, you need to add the task configuration as shown here: generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } NOTE: Please use the latest version of the plugin, available here The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. Note that on a Kotlin project, the generateJava task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin.","title":"Quick Start"},{"location":"generating-code-from-schema/#mapping-existing-types","text":"Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Date and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] }","title":"Mapping existing types"},{"location":"generating-code-from-schema/#generating-client-apis","text":"The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the QueryExecutor . The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *ProjectionRoot . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick }","title":"Generating Client APIs"},{"location":"generating-code-from-schema/#generating-query-apis-for-external-services","text":"Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client . When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API. generateJava { schemaPaths = [ \"${projectDir}/composed-schema.graphqls\" ] packageName = \"some.other.service\" generateClient = true generateDataTypes = false skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] shortProjectionNames = true maxProjectionDepth = 2 }","title":"Generating Query APIs for external services"},{"location":"generating-code-from-schema/#configuring-code-generation","text":"Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well. Configuration property Description Default Value schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types language Either java or kotlin Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types for primitives false (boxed types are used only for nullable fields) generateClient Generate a Query API false generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generatedSourcesDir Build directory for Gradle build outputDir Sub directory of the generatedSourcesDir to generate into generated exampleOutputDir Directory to generate datafetcher example code to generated-examples includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema skipEntityQueries Disable generating Entity queries for federated types false shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false maxProjectionDepth Maximum projection depth to generate. Useful for (federated) schemas with very deep nesting 10","title":"Configuring code generation"},{"location":"getting-started/","text":"Create a new Spring Boot application The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended). Adding the DGS Framework Dependency Add the com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter dependency to your Gradle or Maven configuration. dgs version: Gradle repositories { mavenCentral () } dependencies { implementation \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" } Gradle Kotlin repositories { mavenCentral () } dependencies { implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" ) } Maven <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> <!-- Make sure to set the latest framework version! --> <version> ${dgs.framework.version} </version> </dependency> NOTE: The DGS Framework requires Kotlin 1.4, and does not work with Kotlin 1.3. Older Spring Boot versions may bring in Kotlin 1.3. Creating a Schema The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title. Implement a Data Fetcher Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested! Using @InputArgument You may have noticed the use of @InputArgument to extract the input arguments from your data fetching environment. This should work for most input types, such as String , Integer , custom scalars, and input objects. Java @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = DgsConstants . MUTATION . AddReview ) public List < Review > addReview ( @InputArgument ( \"review\" ) SubmittedReview reviewInput ) { reviewsService . saveReview ( reviewInput ); List < Review > reviews = reviewsService . reviewsForShow ( reviewInput . getShowId ()); return Objects . requireNonNullElseGet ( reviews , List :: of ); } The above is applicable for most list types representing scalars or custom scalars, such as, List<Integer> , List<String> , List<DateTime> etc. However, if you have a list of input object types, you will also need to specify the collection type for proper deserialization as shown below: Java @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = DgsConstants . MUTATION . AddReviews ) public List < Review > addReviews ( @InputArgument ( value = \"reviews\" , collectionType = SubmittedReview . class ) List < SubmittedReview > reviewsInput ) { reviewsService . saveReviews ( reviewsInput ); List < Integer > showIds = reviewsInput . stream (). map ( review -> review . getShowId () ). collect ( Collectors . toList ()); Map < Integer , List < Review >> reviews = reviewsService . reviewsForShows ( showIds ); return new ArrayList ( reviews . values ()); } Test the app with GraphiQL Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client . Next steps Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Learn more about datafetchers Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit Look at example projects","title":"Getting Started"},{"location":"getting-started/#create-a-new-spring-boot-application","text":"The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended).","title":"Create a new Spring Boot application"},{"location":"getting-started/#adding-the-dgs-framework-dependency","text":"Add the com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter dependency to your Gradle or Maven configuration. dgs version: Gradle repositories { mavenCentral () } dependencies { implementation \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" } Gradle Kotlin repositories { mavenCentral () } dependencies { implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" ) } Maven <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> <!-- Make sure to set the latest framework version! --> <version> ${dgs.framework.version} </version> </dependency> NOTE: The DGS Framework requires Kotlin 1.4, and does not work with Kotlin 1.3. Older Spring Boot versions may bring in Kotlin 1.3.","title":"Adding the DGS Framework Dependency"},{"location":"getting-started/#creating-a-schema","text":"The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title.","title":"Creating a Schema"},{"location":"getting-started/#implement-a-data-fetcher","text":"Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested!","title":"Implement a Data Fetcher"},{"location":"getting-started/#using-inputargument","text":"You may have noticed the use of @InputArgument to extract the input arguments from your data fetching environment. This should work for most input types, such as String , Integer , custom scalars, and input objects. Java @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = DgsConstants . MUTATION . AddReview ) public List < Review > addReview ( @InputArgument ( \"review\" ) SubmittedReview reviewInput ) { reviewsService . saveReview ( reviewInput ); List < Review > reviews = reviewsService . reviewsForShow ( reviewInput . getShowId ()); return Objects . requireNonNullElseGet ( reviews , List :: of ); } The above is applicable for most list types representing scalars or custom scalars, such as, List<Integer> , List<String> , List<DateTime> etc. However, if you have a list of input object types, you will also need to specify the collection type for proper deserialization as shown below: Java @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = DgsConstants . MUTATION . AddReviews ) public List < Review > addReviews ( @InputArgument ( value = \"reviews\" , collectionType = SubmittedReview . class ) List < SubmittedReview > reviewsInput ) { reviewsService . saveReviews ( reviewsInput ); List < Integer > showIds = reviewsInput . stream (). map ( review -> review . getShowId () ). collect ( Collectors . toList ()); Map < Integer , List < Review >> reviews = reviewsService . reviewsForShows ( showIds ); return new ArrayList ( reviews . values ()); }","title":"Using @InputArgument"},{"location":"getting-started/#test-the-app-with-graphiql","text":"Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client .","title":"Test the app with GraphiQL"},{"location":"getting-started/#next-steps","text":"Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Learn more about datafetchers Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit Look at example projects","title":"Next steps"},{"location":"mutations/","text":"The DGS framework supports Mutations with the same constructs as data fetchers, using the @DgsData annotation. The following is a simple example of a mutation: type Mutation { addRating(title: String, stars: Int):Rating } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { int stars = dataFetchingEnvironment . getArgument ( \"stars\" ); if ( stars < 1 ) { throw new IllegalArgumentException ( \"Stars must be 1-5\" ); } String title = dataFetchingEnvironment . getArgument ( \"title\" ); System . out . println ( \"Rated \" + title + \" with \" + stars + \" stars\" ) ; return new Rating ( stars ); } } Note that the code above retrieves the input data for the Mutation by calling the DataFetchingEnvironment.getArgument method, just as data fetchers do for their arguments. Input Types In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } } Input arguments as data fetcher method parameters The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } } Kotlin data types In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput ( var title : String , var stars : Int )","title":"Mutations"},{"location":"mutations/#input-types","text":"In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } }","title":"Input Types"},{"location":"mutations/#input-arguments-as-data-fetcher-method-parameters","text":"The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } }","title":"Input arguments as data fetcher method parameters"},{"location":"mutations/#kotlin-data-types","text":"In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput ( var title : String , var stars : Int )","title":"Kotlin data types"},{"location":"query-execution-testing/","text":"The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries. Example Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test. Building GraphQL Queries for Tests In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >> ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods. Mocking External Service Calls in Tests It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests. Example Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } } Testing Exceptions The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ] . message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection.","title":"Testing"},{"location":"query-execution-testing/#example","text":"Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test.","title":"Example"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","text":"In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >> ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.","title":"Building GraphQL Queries for Tests"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","text":"It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests.","title":"Mocking External Service Calls in Tests"},{"location":"query-execution-testing/#example_1","text":"Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } }","title":"Example"},{"location":"query-execution-testing/#testing-exceptions","text":"The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ] . message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection.","title":"Testing Exceptions"},{"location":"scalars/","text":"It is easy to add a custom scalar type in the DGS framework: Create a class that implements the graphql.schema.Coercing interface and annotate it with the @DgsScalar annotation. Also make sure the scalar type is defined in your [GraphQL] schema! For example, this is a simple LocalDateTime implementation: @DgsScalar ( name = \"DateTime\" ) public class DateTimeScalar implements Coercing < LocalDateTime , String > { @Override public String serialize ( Object dataFetcherResult ) throws CoercingSerializeException { if ( dataFetcherResult instanceof LocalDateTime ) { return (( LocalDateTime ) dataFetcherResult ). format ( DateTimeFormatter . ISO_DATE_TIME ); } else { throw new CoercingSerializeException ( \"Not a valid DateTime\" ); } } @Override public LocalDateTime parseValue ( Object input ) throws CoercingParseValueException { return LocalDateTime . parse ( input . toString (), DateTimeFormatter . ISO_DATE_TIME ); } @Override public LocalDateTime parseLiteral ( Object input ) throws CoercingParseLiteralException { if ( input instanceof StringValue ) { return LocalDateTime . parse ((( StringValue ) input ). getValue (), DateTimeFormatter . ISO_DATE_TIME ); } throw new CoercingParseLiteralException ( \"Value is not a valid ISO date time\" ); } } Schema: scalar DateTime Registering Custom Scalars In more recent versions of graphql-java (>v15.0), some scalars, most notably, the Long scalar are no longer available by default. These are non standard scalar that is difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to 1) Define the scalar in your schema, and 2) Register the scalar. Here is an example of how you would set that up: Schema: scalar Long You can register the Long scalar manually with the DGS Framework as shown here: @DgsComponent public class LongScalarRegistration { @DgsRuntimeWiring public RuntimeWiring . Builder addScalar ( RuntimeWiring . Builder builder ) { return builder . scalar ( Scalars . GraphQLLong ); } }","title":"Adding Custom Scalars"},{"location":"scalars/#registering-custom-scalars","text":"In more recent versions of graphql-java (>v15.0), some scalars, most notably, the Long scalar are no longer available by default. These are non standard scalar that is difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to 1) Define the scalar in your schema, and 2) Register the scalar. Here is an example of how you would set that up: Schema: scalar Long You can register the Long scalar manually with the DGS Framework as shown here: @DgsComponent public class LongScalarRegistration { @DgsRuntimeWiring public RuntimeWiring . Builder addScalar ( RuntimeWiring . Builder builder ) { return builder . scalar ( Scalars . GraphQLLong ); } }","title":"Registering Custom Scalars"},{"location":"advanced/context-passing/","text":"Commonly, the datafetcher for a nested field requires properties from its parent object to load its data. Take the following schema example. type Query { shows: [Show] } type Show { # The showId may or may not be there, depending on the scenario. showId: ID title: String reviews: [Review] } type Review { starRating: Int } Let's assume our backend already has methods available to Shows and Reviews from a datastore. Note that for this example, the getShows method does not return reviews. The getReviewsForShow method loads reviews for a show, given the show id. interface ShowsService { List < Show > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } For this scenario, you likely want to have two datafetchers, one for shows and one for reviews. There are different options for implementing the datafetcher, which each has pros and cons depending on the scenario. We'll go over the different scenarios and options. The easy case - Using getSource In the example schema the Show type has a showId . Having the showId available makes loading reviews in a separate datafetcher very easy. The DataFetcherEnvironment has a getSource() method that returns the parent loaded for a field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (); } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); return showsService . getReviewsForShow ( show . getShowId ()); } This example is the easiest and most common scenario, but only possible if the showId field is available on the Show type. No showdId - Use an internal type Sometimes you don't want to expose the showId field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the showId available on Show . If we remove showId from the schema and use codegen, the generated Show type will not have showId field either. Not having the showId field makes loading reviews a bit more complicated, because now we can't get the showId from the Show type using getSource() . The getShowsForService(int showId) method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of Show than exposed in the API. For the remainder of the example, we'll call this the InternalShow type which the ShowsService returns. interface ShowsService { List < InternalShow > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } class InternalShow { int showId ; Sring title ; // getters and setters } However, the Show type in the GraphQL schema does not have a showId . type Show { title: String reviews: [Review] } The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response. We could create an extra ShowWithId wrapper class that either extends or composes the (generated) Show type, and adds a showId field. class ShowWithId { String showId ; Show show ; //Delegate all show fields String getTitle () { return show . getTitle (); } static ShowWithId fromInternalShow ( InternalShow internal ) { //Create Show instance and store id. } .... } The shows datafetcher should return the wrapper type instead of just Show . @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (). stream () . map ( ShowWithId :: fromInternalShow ) . collect ( Collectors . toList ()); } As said, the extra field doesn't affect the response to the client at all. No showId - use local context Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a DataFetcherResult<T> , which contains data , errors and localContext . The data and errors fields are the data and errors you would normally return directly from your datafetcher. The localContext field can hold any data you want to pass down to child datafetchers. The localContext can be retrieved in the child datafetcher from the DataFetchingEnvironment and is passed down to the next level child datafetchers if not overwritten. In the following example the shows datafetcher creates a DataFetcherResult that holds the list of Show instances (not the internal type). The localContext is set to a map with each show as key, and the showId as value. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { List < InternalShow > internalShows = getShows ( titleFilter ); List < Show > shows = internalShows . stream () . map ( s -> Show . newBuilder (). title ( s . getTitle ()). build ()) . collect ( Collectors . toList ()); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( internalShows . stream () . collect ( Collectors . toMap ( s -> Show . newBuilder (). title ( s . getTitle ()). build (), InternalShow :: getId ))) . build (); } private List < InternalShow > getShows ( String titleFilter ) { if ( titleFilter == null ) { return showsService . shows (); } return showsService . shows (). stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } The reviews datafetcher can now use a combination of the getSource and getLocalContext methods to get the showId for a show. @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public CompletableFuture < List < Review >> reviews ( DgsDataFetchingEnvironment dfe ) { Map < Show , Integer > shows = dfe . getLocalContext (); Show show = dfe . getSource (); return showsService . getReviewsForShow ( shows . get ( show )); } A benefit of this approach is that in contrast with getSource , the localContext gets passed down to the next level of child datafechers as well. Pre-loading Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query. In that case, it can be more efficient to pre-load reviews in the shows datafetcher. In the shows datafetcher we can check if the reviews field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the Show type may or may not have a reviews field. If we use DGS codegen it will, and we can just set the reviews field when creating the Show instances in the shows datafetcher. If the type returned by the shows datafetcher does not have a reviews field, we can again use the localContext to pass on the review data to a reviews datafetcher. Below is an example of pre-loading and using localContext . @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( DataFetchingEnvironment dfe ) { List < Show > shows = showsService . shows (); if ( dfe . getSelectionSet (). contains ( \"reviews\" )) { Map < Integer , List < Review >> reviewsForShows = reviewsService . reviewsForShows ( shows . stream (). map ( Show :: getId ). collect ( Collectors . toList ())); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( reviewsForShows ) . build (); } else { return DataFetcherResult . < List < Show >> newResult (). data ( shows ). build (); } } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); //Load the reviews from the pre-loaded localContext. Map < Integer , List < Review >> reviewsForShows = dfe . getLocalContext (); return reviewsForShows . get ( show . getId ()); }","title":"Nested data fetchers"},{"location":"advanced/context-passing/#the-easy-case-using-getsource","text":"In the example schema the Show type has a showId . Having the showId available makes loading reviews in a separate datafetcher very easy. The DataFetcherEnvironment has a getSource() method that returns the parent loaded for a field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (); } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); return showsService . getReviewsForShow ( show . getShowId ()); } This example is the easiest and most common scenario, but only possible if the showId field is available on the Show type.","title":"The easy case - Using getSource"},{"location":"advanced/context-passing/#no-showdid-use-an-internal-type","text":"Sometimes you don't want to expose the showId field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the showId available on Show . If we remove showId from the schema and use codegen, the generated Show type will not have showId field either. Not having the showId field makes loading reviews a bit more complicated, because now we can't get the showId from the Show type using getSource() . The getShowsForService(int showId) method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of Show than exposed in the API. For the remainder of the example, we'll call this the InternalShow type which the ShowsService returns. interface ShowsService { List < InternalShow > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } class InternalShow { int showId ; Sring title ; // getters and setters } However, the Show type in the GraphQL schema does not have a showId . type Show { title: String reviews: [Review] } The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response. We could create an extra ShowWithId wrapper class that either extends or composes the (generated) Show type, and adds a showId field. class ShowWithId { String showId ; Show show ; //Delegate all show fields String getTitle () { return show . getTitle (); } static ShowWithId fromInternalShow ( InternalShow internal ) { //Create Show instance and store id. } .... } The shows datafetcher should return the wrapper type instead of just Show . @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (). stream () . map ( ShowWithId :: fromInternalShow ) . collect ( Collectors . toList ()); } As said, the extra field doesn't affect the response to the client at all.","title":"No showdId - Use an internal type"},{"location":"advanced/context-passing/#no-showid-use-local-context","text":"Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a DataFetcherResult<T> , which contains data , errors and localContext . The data and errors fields are the data and errors you would normally return directly from your datafetcher. The localContext field can hold any data you want to pass down to child datafetchers. The localContext can be retrieved in the child datafetcher from the DataFetchingEnvironment and is passed down to the next level child datafetchers if not overwritten. In the following example the shows datafetcher creates a DataFetcherResult that holds the list of Show instances (not the internal type). The localContext is set to a map with each show as key, and the showId as value. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { List < InternalShow > internalShows = getShows ( titleFilter ); List < Show > shows = internalShows . stream () . map ( s -> Show . newBuilder (). title ( s . getTitle ()). build ()) . collect ( Collectors . toList ()); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( internalShows . stream () . collect ( Collectors . toMap ( s -> Show . newBuilder (). title ( s . getTitle ()). build (), InternalShow :: getId ))) . build (); } private List < InternalShow > getShows ( String titleFilter ) { if ( titleFilter == null ) { return showsService . shows (); } return showsService . shows (). stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } The reviews datafetcher can now use a combination of the getSource and getLocalContext methods to get the showId for a show. @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public CompletableFuture < List < Review >> reviews ( DgsDataFetchingEnvironment dfe ) { Map < Show , Integer > shows = dfe . getLocalContext (); Show show = dfe . getSource (); return showsService . getReviewsForShow ( shows . get ( show )); } A benefit of this approach is that in contrast with getSource , the localContext gets passed down to the next level of child datafechers as well.","title":"No showId - use local context"},{"location":"advanced/context-passing/#pre-loading","text":"Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query. In that case, it can be more efficient to pre-load reviews in the shows datafetcher. In the shows datafetcher we can check if the reviews field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the Show type may or may not have a reviews field. If we use DGS codegen it will, and we can just set the reviews field when creating the Show instances in the shows datafetcher. If the type returned by the shows datafetcher does not have a reviews field, we can again use the localContext to pass on the review data to a reviews datafetcher. Below is an example of pre-loading and using localContext . @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( DataFetchingEnvironment dfe ) { List < Show > shows = showsService . shows (); if ( dfe . getSelectionSet (). contains ( \"reviews\" )) { Map < Integer , List < Review >> reviewsForShows = reviewsService . reviewsForShows ( shows . stream (). map ( Show :: getId ). collect ( Collectors . toList ())); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( reviewsForShows ) . build (); } else { return DataFetcherResult . < List < Show >> newResult (). data ( shows ). build (); } } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); //Load the reviews from the pre-loaded localContext. Map < Integer , List < Review >> reviewsForShows = dfe . getLocalContext (); return reviewsForShows . get ( show . getId ()); }","title":"Pre-loading"},{"location":"advanced/custom-datafetcher-context/","text":"Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling DataFetchingEnvironment.getContext() . This is a common mechanism to pass request context to data fetchers and data loaders. The DGS framework has its own DgsContext implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context. To create a custom context, implement a Spring bean of type DgsCustomContextBuilder . Write the build() method so that it creates an instance of the type that represents your custom context object: @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Data Fetching Context"},{"location":"advanced/dynamic-schemas/","text":"We strongly recommend primarily using schema-first development. Most DGSs have a schema file and use the declarative, annotation-based programming model to create data fetchers and such. That said, there are scenarios where generating the schema from another source, possibly dynamically, is required. Creating a schema from code Create a schema from code by using the @DgsTypeDefinitionFactory annotation. Use the @DgsTypeDefinitionFactory on methods inside a @DgsComponent class to provide a TypeDefinitionFactory . The TypeDefinitionFactory is part of the graphql-java API. You use a TypeDefinitionFactory to programmatically define a schema. Note that you can mix static schema files with one or more DgsTypeDefinitionFactory methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to @DgsTypeDefinitionFactory to add some dynamic parts to the schema. The following is an example of a DgsTypeDefinitionFactory . @DgsComponent public class DynamicTypeDefinitions { @DgsTypeDefinitionRegistry public TypeDefinitionRegistry registry () { TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); ObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"randomNumber\" ). type ( new TypeName ( \"Int\" )). build () ). build (); typeDefinitionRegistry . add ( query ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field randomNumber on the Query object type. Creating datafetchers programmatically If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the @DgsCodeRegistry annotation to add datafetchers programmatically. A method annotated @DgsCodeRegistry takes two arguments: GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry The method must return the modified GraphQLCodeRegistry.Builder. The following is an example of a programmatically created datafetcher for the field created in the previous example. @DgsComponent public class DynamicDataFetcher { @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < Integer > df = ( dfe ) -> new Random (). nextInt (); FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"randomNumber\" ); return codeRegistryBuilder . dataFetcher ( coordinates , df ); } } Changing schemas at runtime It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own ReloadSchemaIndicator . You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the @DgsTypeDefinitionFactory and @DgsCodeRegistry again. If these methods create the schema based on external input, you have a system that can dynamically rewire its API. For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!","title":"Dynamic schemas"},{"location":"advanced/dynamic-schemas/#creating-a-schema-from-code","text":"Create a schema from code by using the @DgsTypeDefinitionFactory annotation. Use the @DgsTypeDefinitionFactory on methods inside a @DgsComponent class to provide a TypeDefinitionFactory . The TypeDefinitionFactory is part of the graphql-java API. You use a TypeDefinitionFactory to programmatically define a schema. Note that you can mix static schema files with one or more DgsTypeDefinitionFactory methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to @DgsTypeDefinitionFactory to add some dynamic parts to the schema. The following is an example of a DgsTypeDefinitionFactory . @DgsComponent public class DynamicTypeDefinitions { @DgsTypeDefinitionRegistry public TypeDefinitionRegistry registry () { TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); ObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"randomNumber\" ). type ( new TypeName ( \"Int\" )). build () ). build (); typeDefinitionRegistry . add ( query ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field randomNumber on the Query object type.","title":"Creating a schema from code"},{"location":"advanced/dynamic-schemas/#creating-datafetchers-programmatically","text":"If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the @DgsCodeRegistry annotation to add datafetchers programmatically. A method annotated @DgsCodeRegistry takes two arguments: GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry The method must return the modified GraphQLCodeRegistry.Builder. The following is an example of a programmatically created datafetcher for the field created in the previous example. @DgsComponent public class DynamicDataFetcher { @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < Integer > df = ( dfe ) -> new Random (). nextInt (); FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"randomNumber\" ); return codeRegistryBuilder . dataFetcher ( coordinates , df ); } }","title":"Creating datafetchers programmatically"},{"location":"advanced/dynamic-schemas/#changing-schemas-at-runtime","text":"It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own ReloadSchemaIndicator . You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the @DgsTypeDefinitionFactory and @DgsCodeRegistry again. If these methods create the schema based on external input, you have a system that can dynamically rewire its API. For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!","title":"Changing schemas at runtime"},{"location":"advanced/federated-testing/","text":"Federation allows you to extend or reference existing types in a graph. Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. Testing Federated Queries without the Gateway You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Using the Entities Query Builder API Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Federated Testing"},{"location":"advanced/federated-testing/#testing-federated-queries-without-the-gateway","text":"You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); }","title":"Testing Federated Queries without the Gateway"},{"location":"advanced/federated-testing/#using-the-entities-query-builder-api","text":"Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Using the Entities Query Builder API"},{"location":"advanced/file-uploads/","text":"In GraphQL, you model a file upload operation as a GraphQL mutation request from a client to your DGS. The following sections describe how you implement file uploads and downloads using a Multipart POST request. For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog . Multipart File Upload A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"File Uploads"},{"location":"advanced/file-uploads/#multipart-file-upload","text":"A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"Multipart File Upload"},{"location":"advanced/instrumentation/","text":"Adding instrumentation for tracing and logging It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*> , parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*> ) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher ' $ dataFetcherName ' took ${ totalTime } ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher ' $ dataFetcherName ': ${ totalTime } ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${ totalTime } ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \" ${ parentType . name } . ${ parameters . executionStepInfo . path . segmentName } \" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms","title":"Instrumentation (Tracing, Metrics)"},{"location":"advanced/instrumentation/#adding-instrumentation-for-tracing-and-logging","text":"It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*> , parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*> ) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher ' $ dataFetcherName ' took ${ totalTime } ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher ' $ dataFetcherName ': ${ totalTime } ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${ totalTime } ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \" ${ parentType . name } . ${ parameters . executionStepInfo . path . segmentName } \" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms","title":"Adding instrumentation for tracing and logging"},{"location":"advanced/java-client/","text":"Usage The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders HTTP client wrapper The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient ( url ); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate . private RestTemplate dgsRestTemplate; private static final String URL = \"http://someserver/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext() Errors The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" ) Type safe Query API Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination. Interface projections When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); ); Building Federated Queries You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables ();","title":"Java GraphQL Client"},{"location":"advanced/java-client/#usage","text":"The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders","title":"Usage"},{"location":"advanced/java-client/#http-client-wrapper","text":"The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient ( url ); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate . private RestTemplate dgsRestTemplate; private static final String URL = \"http://someserver/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext()","title":"HTTP client wrapper"},{"location":"advanced/java-client/#errors","text":"The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" )","title":"Errors"},{"location":"advanced/java-client/#type-safe-query-api","text":"Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination.","title":"Type safe Query API"},{"location":"advanced/java-client/#interface-projections","text":"When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); );","title":"Interface projections"},{"location":"advanced/java-client/#building-federated-queries","text":"You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables ();","title":"Building Federated Queries"},{"location":"advanced/mocking/","text":"This guide is about how to provide mock data for data fetchers. There are two primary reasons to do so: Provide example data that UI teams can use while the data fetcher is under development. This is useful during schema design. Provide stable test data for UI teams to write their tests against. An argument can be made that this type of mock data should live in the UI code. It\u2019s for their tests after all. However, by pulling it into the DGS, the owners of the data can provide test data that can be used by many teams. The two approaches are also not mutually exclusive. [GraphQL] Mocking The library in the DGS framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key. Example Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely.","title":"Mocking"},{"location":"advanced/mocking/#graphql-mocking","text":"The library in the DGS framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key.","title":"[GraphQL] Mocking"},{"location":"advanced/mocking/#example","text":"Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely.","title":"Example"},{"location":"advanced/schema-reloading/","text":"The DGS framework is designed to work well with tools such as JRebel. In large Spring Boot codebases with many dependencies, it can take some time to restart the application during development. Waiting for the application to start can be disruptive to the development workflow. Enabling development mode for hot reloading Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup. We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways: Set the dgs.reload configuration property to true (e.g. in application.yml ) Enable the laptop profile Implement your own ReloadIndicator bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas .","title":"Hot reloading schemas"},{"location":"advanced/schema-reloading/#enabling-development-mode-for-hot-reloading","text":"Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup. We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways: Set the dgs.reload configuration property to true (e.g. in application.yml ) Enable the laptop profile Implement your own ReloadIndicator bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas .","title":"Enabling development mode for hot reloading"},{"location":"advanced/security/","text":"Fine-grained Access Control with @Secured The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Security"},{"location":"advanced/security/#fine-grained-access-control-with-secured","text":"The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Fine-grained Access Control with @Secured"},{"location":"advanced/subscriptions/","text":"GraphQL Subscriptions are used to receive updates for a query from the server over time. A common example is sending update notifications from the server. Regular GraphQL queries use a simple (HTTP) request/response to execute a query. For subscriptions a connection is kept open. Currently, we support subscriptions using Websockets. We will add support for SSE in the future. The Server Side Programming Model In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; \u22ee @DgsData ( parentType = \"Subscription\" , field = \"stocks\" ) public Publisher < Stock > stocks () { return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Tick ( t . toString ()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.java . Next, a transport implementation must be chosen , which depends on how your app is deployed . WebSockets The subscription endpoint is on /subscriptions . Normal GraphQL queries can be sent to /graphql , while subscription requests go to /subscriptions . The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically, you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.","title":"Subscriptions"},{"location":"advanced/subscriptions/#the-server-side-programming-model","text":"In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; \u22ee @DgsData ( parentType = \"Subscription\" , field = \"stocks\" ) public Publisher < Stock > stocks () { return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Tick ( t . toString ()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.java . Next, a transport implementation must be chosen , which depends on how your app is deployed .","title":"The Server Side Programming Model"},{"location":"advanced/subscriptions/#websockets","text":"The subscription endpoint is on /subscriptions . Normal GraphQL queries can be sent to /graphql , while subscription requests go to /subscriptions . The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically, you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.","title":"WebSockets"},{"location":"advanced/type-resolvers-for-abstract-types/","text":"You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation . As an example, the following schema defines a Movie interface type with two different concrete object type implementations. type Query { movies: [Movie] } interface Movie { title: String } type ScaryMovie implements Movie { title: String gory: Boolean scareFactor: Int } type ActionMovie implements Movie { title: String nrOfExplosions: Int } The following data fetcher is registered to return a list of movies. The data fetcher returns a combination Movie types. @DgsComponent public class MovieDataFetcher { @DgsData ( parentType = \"Query\" , field = \"movies\" ) public List < Movie > movies () { return Lists . newArrayList ( new ActionMovie ( \"Crouching Tiger\" , 0 ), new ActionMovie ( \"Black hawk down\" , 10 ), new ScaryMovie ( \"American Horror Story\" , true , 10 ), new ScaryMovie ( \"Love Death + Robots\" , false , 4 ) ); } } The GraphQL runtime needs to know that a Java instance of ActionMovie represents the ActionMovie GraphQL type. This mapping is the responsibility of a TypeResolver . Tip: If your Java type names and GraphQL type names are the same, the DGS framework creates a `TypeResolver` automatically. No code needs to be added! Registering a Type Resolver If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Interfaces and Unions"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","text":"If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Registering a Type Resolver"}]}